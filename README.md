ğŸ’¼ Know-Churn
An end-to-end churn prediction system built with CatBoost, orchestrated using ZenML, tracked via MLflow, and served through a sleek Streamlit app. Model experimentation was done in Jupyter, and the final model is stored and reused during inference.
 
ğŸ§  Project Highlights
â€¢	ğŸ” Jupyter notebook for prototyping and model selection
â€¢	ğŸ“¦ ZenML pipelines for training and inference workflows
â€¢	ğŸ§® CatBoostClassifier for final model due to native categorical support
â€¢	ğŸ“Š MLflow used for experiment tracking and artifact logging
â€¢	ğŸŒ Streamlit frontend for live predictions and user interaction
â€¢	ğŸ“ Structured folder system for clarity and scalability
 
ğŸ“ Project Structure
customer_churn/
â”œâ”€â”€ data/                        # Raw data
â”‚   â””â”€â”€ telco_churn.csv
â”œâ”€â”€ jupyter/
â”‚   â””â”€â”€ prototyping.ipynb        # Notebook for model experiments
â”œâ”€â”€ pipelines/
â”‚   â”œâ”€â”€ inference_pipeline.py
â”‚   â””â”€â”€ training_pipeline.py
â”œâ”€â”€ saved_model/
â”‚   â”œâ”€â”€ cat_boost_model.pkl      # Final trained model
â”‚   â””â”€â”€ encoder.pkl              # Trained encoder
â”œâ”€â”€ src/                         # Core business logic
â”‚   â”œâ”€â”€ data_cleaning/
â”‚   â”œâ”€â”€ encoder/
â”‚   â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ model_dev/
â”‚   â””â”€â”€ prediction/
â”œâ”€â”€ steps/                       # ZenML steps
â”‚   â”œâ”€â”€ clean_data.py
â”‚   â”œâ”€â”€ Config.py                # Config and parameters
â”‚   â”œâ”€â”€ evaluate_model.py
â”‚   â”œâ”€â”€ ingest_data.py
â”‚   â”œâ”€â”€ load_input_data.py       # For inference
â”‚   â”œâ”€â”€ load_model.py            # Load saved .pkl model
â”‚   â”œâ”€â”€ predict_model.py
â”‚   â”œâ”€â”€ run_inference.py
â”‚   â””â”€â”€ train_model.py
â”œâ”€â”€ streamlit/
â”‚   â””â”€â”€ app.py                   # Streamlit user interface
â”œâ”€â”€ run_inference_pipeline.py    # Entrypoint to run inference
â”œâ”€â”€ run_train_pipeline.py        # Entrypoint to run training
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
 
ğŸ§° Tech Stack
Purpose	Tool
Model	       CatBoost
Workflow Orchestration	       ZenML
Experiment Tracking	       MLflow
Notebook Prototyping	       Jupyter
UI	       Streamlit
Data Handling	      Pandas, Scikit-learn
 
ğŸ”§ Getting Started
1. Clone the repository
git clone https://github.com/spotinforex/customer_churn.git
cd customer_churn
2. Set up virtual environment
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
3. Install dependencies
pip install -r requirements.txt
4. Initialize ZenML and MLflow
zenml init
mlflow ui
Visit http://localhost:5000 to view your experiment logs.
 
ğŸ“Š Prototyping & Model Selection
Run the notebook:
jupyter notebook jupyter/prototyping.ipynb
The notebook explores:
â€¢	Data distribution
â€¢	Handling missing and categorical values
â€¢	Multiple model comparisons
â€¢	Final choice: CatBoostClassifier
 
ğŸ§ª Training the Model
python run_train_pipeline.py
This will:
â€¢	Ingest and clean data
â€¢	Train CatBoost
â€¢	Save model to savedmodel/
â€¢	Log all artifacts to MLflow
 
ğŸ” Run Inference
python run_inference_pipeline.py
â€¢	Loads saved model and encoder
â€¢	Predicts churn on new input (batch or single)
â€¢	Returns results and logs inference if configured
 
ğŸ–¥ï¸ Launch Streamlit App
streamlit run streamlit/app.py
What it offers:
â€¢	Upload CSV for bulk predictions
â€¢	Manually enter data for real-time prediction
â€¢	View churn distribution as a bar chart if added to evaluate model
 
ğŸ“ˆ MLflow Logging
Each training run logs:
â€¢	Metrics (Accuracy, ROC-AUC, Precision, Recall)
â€¢	Parameters
â€¢	Model artifacts (CatBoost + encoder)
â€¢	Versioning across runs
Logs are stored in the mlruns/ directory and viewable via mlflow ui
 
âœ… To-Do / Future Improvements
â€¢	[ ] SHAP explainability visualizations in Streamlit
â€¢	[ ] Switch to cloud storage for model and artifacts
â€¢	[ ] Add user authentication to the app
 
âœï¸ Author
Praisejah Nwabeke
Data Scientist & Builder & Public Administrator
ğŸ“§ nwabekepraisejah@gmail.com
 
ğŸ“œ License
This project is licensed under the MIT License.
